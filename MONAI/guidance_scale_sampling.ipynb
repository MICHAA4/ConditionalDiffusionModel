{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21394f42-5fbe-4b2e-8bb3-cb4d6defae51",
   "metadata": {},
   "source": [
    "## Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe2b6e6a-2aad-4770-9fbe-34fd5e1dc972",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"import monai\" || pip install -q \"monai-weekly[tqdm]\"\n",
    "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f0a006-5891-463e-ad53-f1c350c2989f",
   "metadata": {},
   "source": [
    "## Setup Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "891f2921-5816-4cf5-858c-e516f543a956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.3.1rc2+1.g87152d10\n",
      "Numpy version: 1.26.4\n",
      "Pytorch version: 2.2.2+cu121\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: 87152d106557bdc37de0ee923be4e9a25c4c003c\n",
      "MONAI __file__: /data/<username>/jupyter/MONAI/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: 0.4.11\n",
      "ITK version: 5.3.0\n",
      "Nibabel version: 5.2.1\n",
      "scikit-image version: 0.22.0\n",
      "scipy version: 1.11.4\n",
      "Pillow version: 10.2.0\n",
      "Tensorboard version: 2.16.2\n",
      "gdown version: 4.7.3\n",
      "TorchVision version: 0.17.2+cu121\n",
      "tqdm version: 4.65.0\n",
      "lmdb version: 1.4.1\n",
      "psutil version: 5.9.0\n",
      "pandas version: 2.1.4\n",
      "einops version: 0.7.0\n",
      "transformers version: 4.39.3\n",
      "mlflow version: 2.11.3\n",
      "pynrrd version: 1.0.0\n",
      "clearml version: 1.15.0\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    Lambdad,\n",
    "    Resized,\n",
    "    Randomizable,\n",
    "    EnsureChannelFirstd,\n",
    "    ScaleIntensityRanged\n",
    ")\n",
    "from monai.config import print_config\n",
    "from monai.utils import first, set_determinism\n",
    "from monai.data import Dataset, CacheDataset, DataLoader\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from generative.inferers import DiffusionInferer\n",
    "from generative.networks.nets import DiffusionModelUNet\n",
    "from generative.networks.schedulers import DDPMScheduler\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed74f635-60e9-4b07-aa6d-d4df84aec3da",
   "metadata": {},
   "source": [
    "## Setup Data Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9caeb808-a81c-4383-ba04-3eaad4e87d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmp78qs_ktm\n"
     ]
    }
   ],
   "source": [
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fa21e8-5b14-4f16-bf08-6e2cf4dbbfbc",
   "metadata": {},
   "source": [
    "## Set Deterministic Training for Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a795a093-2baa-49ac-bfc7-fd3ae66bd94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_determinism(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c529070e-78d7-40e7-af20-6380e8c08463",
   "metadata": {},
   "source": [
    "## CamcanDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fa9eccd-1d9c-444d-9581-021be560058d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CamcanDataset(Randomizable, CacheDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir,\n",
    "        csv_file,\n",
    "        section,\n",
    "        transform=None,\n",
    "        seed=0,\n",
    "        val_frac=0.2,\n",
    "        test_frac=0.2,\n",
    "        cache_num=sys.maxsize,\n",
    "        cache_rate=1.0,\n",
    "        num_workers=0,\n",
    "        progress: bool = True,\n",
    "        condition_prob = 0,\n",
    "    ) -> None:\n",
    "        if not os.path.isdir(root_dir):\n",
    "            raise ValueError(\"Root directory root_dir must be a directory.\")\n",
    "        self.root_dir = root_dir\n",
    "        self.csv_file = csv_file\n",
    "        self.section = section\n",
    "        self.val_frac = val_frac\n",
    "        self.test_frac = test_frac\n",
    "        self.condition_prob = condition_prob\n",
    "        self.set_random_state(seed=seed)\n",
    "\n",
    "        data = self._generate_data_list()\n",
    "\n",
    "        CacheDataset.__init__(\n",
    "            self,\n",
    "            data=data,\n",
    "            transform=transform,\n",
    "            cache_num=cache_num,\n",
    "            cache_rate=cache_rate,\n",
    "            num_workers=num_workers,\n",
    "            progress=progress,\n",
    "        )\n",
    "\n",
    "    def randomize(self, data: np.ndarray) -> None:\n",
    "        self.R.shuffle(data)\n",
    "\n",
    "    def _generate_data_list(self):\n",
    "        datalist = []\n",
    "        with open(self.csv_file, mode='r') as file:\n",
    "            reader = csv.DictReader(file)\n",
    "            for row in reader:\n",
    "                image_path = os.path.join(self.root_dir, f\"sub-{row['Subject']}_defaced_T1.nii.gz\")\n",
    "                if not os.path.exists(image_path):\n",
    "                    continue\n",
    "                img = nib.load(image_path)\n",
    "                img_data = img.get_fdata()\n",
    "                for slice_idx in range(img_data.shape[2]//2 - 20, img_data.shape[2]//2 + 20):  # Assuming axial slices\n",
    "                    slice_data = img_data[:,:,slice_idx]\n",
    "                    condition = np.array([\n",
    "                        [int(row['Age'])],\n",
    "                        [int(row['Sex'])],\n",
    "                        [slice_idx]\n",
    "                    ]).reshape((1,3)).astype('float32')\n",
    "                    datalist.append({\n",
    "                        \"image\": slice_data,\n",
    "                        \"condition\": condition\n",
    "                    })\n",
    "        \n",
    "        length = len(datalist)\n",
    "        indices = np.arange(length)\n",
    "        self.randomize(indices)\n",
    "\n",
    "        # train, validation, test split\n",
    "        test_length = int(length * self.test_frac)\n",
    "        val_length = int(length * self.val_frac)\n",
    "        if self.section == \"test\":\n",
    "            section_indices = indices[:test_length]\n",
    "        elif self.section == \"validation\":\n",
    "            section_indices = indices[test_length : test_length + val_length]\n",
    "        elif self.section == \"training\":\n",
    "            section_indices = indices[test_length + val_length :]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f'Unsupported section: {self.section}, available options are [\"training\", \"validation\", \"test\"].'\n",
    "            )\n",
    "        return [datalist[i] for i in section_indices]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data[index]\n",
    "\n",
    "        if random.random() < self.condition_prob:\n",
    "            sample[\"condition\"] = np.array([[-1, -1, -1]])\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ec73a0-5dc4-4dec-b93f-9b1347a3696c",
   "metadata": {},
   "source": [
    "## Setup CamcanDataset and Training and Validation DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58c7bb11-05aa-44ba-87a9-6556d9272095",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|███████████████████| 15480/15480 [00:22<00:00, 684.39it/s]\n"
     ]
    }
   ],
   "source": [
    "# Usage example\n",
    "data_dir = \"./dataset_camcan_sy\"\n",
    "csv_file = \"./phenotype.csv\"\n",
    "\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        EnsureChannelFirstd(keys=[\"image\"], channel_dim='no_channel'),\n",
    "        ScaleIntensityRanged(keys=[\"image\"], a_min=0.0, a_max=255.0, b_min=0.0, b_max=1.0),\n",
    "        Lambdad(keys=[\"condition\"], func=lambda x: torch.tensor(x, dtype=torch.float32)),\n",
    "        Resized(keys=[\"image\"], spatial_size=(96,128)),\n",
    "    ])\n",
    "\n",
    "\n",
    "# Training DataLoader\n",
    "train_ds = CamcanDataset(root_dir=data_dir, csv_file=csv_file, transform=train_transforms, section=\"training\", condition_prob=0.2)\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, num_workers=8, persistent_workers=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b363c2c4-892c-4740-86a5-d6b438031e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|█████████████████████| 5160/5160 [00:07<00:00, 703.15it/s]\n"
     ]
    }
   ],
   "source": [
    "val_transforms = Compose(\n",
    "    [\n",
    "        EnsureChannelFirstd(keys=[\"image\"], channel_dim='no_channel'),\n",
    "        ScaleIntensityRanged(keys=[\"image\"], a_min=0.0, a_max=255.0, b_min=0.0, b_max=1.0),\n",
    "        Lambdad(keys=[\"condition\"], func=lambda x: torch.tensor(x, dtype=torch.float32)),\n",
    "        Resized(keys=[\"image\"], spatial_size=(96,128)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Validation DataLoader\n",
    "val_ds = CamcanDataset(root_dir=data_dir, csv_file=csv_file, transform=val_transforms, section=\"validation\")\n",
    "val_loader = DataLoader(val_ds, batch_size=8, shuffle=False, num_workers=8, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb88a2d-000b-409a-b698-6f27ad7a1277",
   "metadata": {},
   "source": [
    "## Define Network, Scheduler, Optimizer and Inferer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14c8ea30-9b88-4152-b3bc-51019c1c00e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model = DiffusionModelUNet(\n",
    "    spatial_dims=2,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    num_channels=(256, 256, 512),\n",
    "    attention_levels=(False, False, True),\n",
    "    num_res_blocks=2,\n",
    "    num_head_channels=(0, 0, 512),\n",
    "    with_conditioning=True,\n",
    "    cross_attention_dim=3,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=2.5e-5)\n",
    "\n",
    "inferer = DiffusionInferer(scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8608f0dc-71bd-4ba0-93e9-1e2ed9bcff93",
   "metadata": {},
   "source": [
    "## Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c52d404c-7d08-4e5e-8303-6ebdbcb71d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274\n"
     ]
    }
   ],
   "source": [
    "# Path to the pretrained model\n",
    "pretrained_model_path = 'pretrained_model_275.pth'\n",
    "\n",
    "# Load the state dictionary\n",
    "state_dict = torch.load(pretrained_model_path, map_location=device)\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# If you have a checkpoint with more information (like optimizer state), use this:\n",
    "checkpoint = torch.load('pretrained_model_checkpoint_275.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "start_epoch = checkpoint['epoch']\n",
    "print(start_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce0512c-b172-439e-93d0-017a6e046f0d",
   "metadata": {},
   "source": [
    "## Guidance Scale = 7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbe3aac7-ca73-4d65-9287-e7991f003f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     37\u001b[0m     noise_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([noise] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 38\u001b[0m     model_output \u001b[38;5;241m=\u001b[39m model(noise_input, timesteps\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mTensor((t,))\u001b[38;5;241m.\u001b[39mto(noise\u001b[38;5;241m.\u001b[39mdevice), context\u001b[38;5;241m=\u001b[39mconditioning)\n\u001b[1;32m     39\u001b[0m     noise_pred_uncond, noise_pred_text \u001b[38;5;241m=\u001b[39m model_output\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     40\u001b[0m     noise_pred \u001b[38;5;241m=\u001b[39m noise_pred_uncond \u001b[38;5;241m+\u001b[39m guidance_scale \u001b[38;5;241m*\u001b[39m (noise_pred_text \u001b[38;5;241m-\u001b[39m noise_pred_uncond)\n",
      "File \u001b[0;32m/data/sycha2000/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/data/sycha2000/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/sycha2000/anaconda3/lib/python3.11/site-packages/generative/networks/nets/diffusion_model_unet.py:1938\u001b[0m, in \u001b[0;36mDiffusionModelUNet.forward\u001b[0;34m(self, x, timesteps, context, class_labels, down_block_additional_residuals, mid_block_additional_residual)\u001b[0m\n\u001b[1;32m   1936\u001b[0m     res_samples \u001b[38;5;241m=\u001b[39m down_block_res_samples[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(upsample_block\u001b[38;5;241m.\u001b[39mresnets) :]\n\u001b[1;32m   1937\u001b[0m     down_block_res_samples \u001b[38;5;241m=\u001b[39m down_block_res_samples[: \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(upsample_block\u001b[38;5;241m.\u001b[39mresnets)]\n\u001b[0;32m-> 1938\u001b[0m     h \u001b[38;5;241m=\u001b[39m upsample_block(hidden_states\u001b[38;5;241m=\u001b[39mh, res_hidden_states_list\u001b[38;5;241m=\u001b[39mres_samples, temb\u001b[38;5;241m=\u001b[39memb, context\u001b[38;5;241m=\u001b[39mcontext)\n\u001b[1;32m   1940\u001b[0m \u001b[38;5;66;03m# 7. output block\u001b[39;00m\n\u001b[1;32m   1941\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout(h)\n",
      "File \u001b[0;32m/data/sycha2000/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/data/sycha2000/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/sycha2000/anaconda3/lib/python3.11/site-packages/generative/networks/nets/diffusion_model_unet.py:1463\u001b[0m, in \u001b[0;36mCrossAttnUpBlock.forward\u001b[0;34m(self, hidden_states, res_hidden_states_list, temb, context)\u001b[0m\n\u001b[1;32m   1460\u001b[0m     res_hidden_states_list \u001b[38;5;241m=\u001b[39m res_hidden_states_list[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1461\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([hidden_states, res_hidden_states], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 1463\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m resnet(hidden_states, temb)\n\u001b[1;32m   1464\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m attn(hidden_states, context\u001b[38;5;241m=\u001b[39mcontext)\n\u001b[1;32m   1466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsampler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/data/sycha2000/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/data/sycha2000/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/sycha2000/anaconda3/lib/python3.11/site-packages/generative/networks/nets/diffusion_model_unet.py:692\u001b[0m, in \u001b[0;36mResnetBlock.forward\u001b[0;34m(self, x, emb)\u001b[0m\n\u001b[1;32m    689\u001b[0m     temb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_emb_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnonlinearity(emb))[:, :, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m    690\u001b[0m h \u001b[38;5;241m=\u001b[39m h \u001b[38;5;241m+\u001b[39m temb\n\u001b[0;32m--> 692\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(h)\n\u001b[1;32m    693\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnonlinearity(h)\n\u001b[1;32m    694\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(h)\n",
      "File \u001b[0;32m/data/sycha2000/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1675\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1666\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m   1668\u001b[0m \u001b[38;5;66;03m# On the return type:\u001b[39;00m\n\u001b[1;32m   1669\u001b[0m \u001b[38;5;66;03m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[1;32m   1670\u001b[0m \u001b[38;5;66;03m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;66;03m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[1;32m   1674\u001b[0m \u001b[38;5;66;03m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[0;32m-> 1675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1677\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "guidance_scale = 7.0\n",
    "\n",
    "generated_images_path = './generated_images/275_7.0'\n",
    "os.makedirs(generated_images_path, exist_ok=True)\n",
    "\n",
    "max_samples = 1500\n",
    "sample_count = 0\n",
    "\n",
    "for batch in val_loader:\n",
    "    if sample_count < 1000:\n",
    "        sample_count += 8\n",
    "        continue\n",
    "    if sample_count >= max_samples:\n",
    "        break\n",
    "\n",
    "    real_images = batch['image'].to(device)\n",
    "    conditions = batch['condition'].to(device)\n",
    "    \n",
    "    for i in range(real_images.size(0)):\n",
    "        if sample_count >= max_samples:\n",
    "            break\n",
    "\n",
    "        age, sex, slice_number = conditions[i][0, 0], conditions[i][0, 1], conditions[i][0, 2]\n",
    "        unconditioned = torch.tensor([[-1, -1, -1]], dtype=torch.float32)  # Shape: (1, 3)\n",
    "        conditioned = torch.tensor([[age, sex, slice_number]], dtype=torch.float32)  # Shape: (1, 3)\n",
    "        conditioning = torch.stack([unconditioned, conditioned], dim=0).to(device)\n",
    "            \n",
    "        noise = torch.randn((1, 1, 96, 128)).to(device)\n",
    "        scheduler.set_timesteps(num_inference_steps=1000)\n",
    "        progress_bar = tqdm(scheduler.timesteps, leave=False)\n",
    "            \n",
    "        for t in progress_bar:\n",
    "            with autocast(enabled=True):\n",
    "                with torch.no_grad():\n",
    "                    noise_input = torch.cat([noise] * 2)\n",
    "                    model_output = model(noise_input, timesteps=torch.Tensor((t,)).to(noise.device), context=conditioning)\n",
    "                    noise_pred_uncond, noise_pred_text = model_output.chunk(2)\n",
    "                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "            noise, _ = scheduler.step(noise_pred, t, noise)\n",
    "            \n",
    "        # Save the generated image\n",
    "        noise_image = noise[0, 0].cpu().numpy()\n",
    "        g_file_name = f\"generated_{sample_count}_{int(slice_number):03d}.png\"\n",
    "        generated_file = os.path.join(generated_images_path, g_file_name)\n",
    "        # Save the image using plt.imsave\n",
    "        plt.imsave(generated_file, noise_image, vmin=0, vmax=1, cmap='gray')\n",
    "        \n",
    "        sample_count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8d6bf0-7e58-4626-9a7d-4b21f5a0b91f",
   "metadata": {},
   "source": [
    "## Guidance Scale = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b634e948-1faf-41c1-a058-62fd57cc4a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "guidance_scale = 5.0\n",
    "\n",
    "generated_images_path = './generated_images/275_5.0' \n",
    "os.makedirs(generated_images_path, exist_ok=True)\n",
    "\n",
    "max_samples = 1500\n",
    "sample_count = 0\n",
    "\n",
    "for batch in val_loader:\n",
    "    if sample_count < 1000:\n",
    "        sample_count += 8\n",
    "        continue\n",
    "    if sample_count >= max_samples:\n",
    "        break\n",
    "\n",
    "    real_images = batch['image'].to(device)\n",
    "    conditions = batch['condition'].to(device)\n",
    "    \n",
    "    for i in range(real_images.size(0)):\n",
    "        if sample_count >= max_samples:\n",
    "            break\n",
    "\n",
    "        age, sex, slice_number = conditions[i][0, 0], conditions[i][0, 1], conditions[i][0, 2]\n",
    "        unconditioned = torch.tensor([[-1, -1, -1]], dtype=torch.float32)  # Shape: (1, 3)\n",
    "        conditioned = torch.tensor([[age, sex, slice_number]], dtype=torch.float32)  # Shape: (1, 3)\n",
    "        conditioning = torch.stack([unconditioned, conditioned], dim=0).to(device)\n",
    "            \n",
    "        noise = torch.randn((1, 1, 96, 128)).to(device)\n",
    "        scheduler.set_timesteps(num_inference_steps=1000)\n",
    "        progress_bar = tqdm(scheduler.timesteps, leave=False)\n",
    "            \n",
    "        for t in progress_bar:\n",
    "            with autocast(enabled=True):\n",
    "                with torch.no_grad():\n",
    "                    noise_input = torch.cat([noise] * 2)\n",
    "                    model_output = model(noise_input, timesteps=torch.Tensor((t,)).to(noise.device), context=conditioning)\n",
    "                    noise_pred_uncond, noise_pred_text = model_output.chunk(2)\n",
    "                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "            noise, _ = scheduler.step(noise_pred, t, noise)\n",
    "            \n",
    "        # Save the generated image\n",
    "        noise_image = noise[0, 0].cpu().numpy()\n",
    "        g_file_name = f\"generated_{sample_count}_{int(slice_number):03d}.png\"\n",
    "        generated_file = os.path.join(generated_images_path, g_file_name)\n",
    "        # Save the image using plt.imsave\n",
    "        plt.imsave(generated_file, noise_image, vmin=0, vmax=1, cmap='gray')\n",
    "        \n",
    "        sample_count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1fa37c-d0e4-48f8-a694-3753c77db8c8",
   "metadata": {},
   "source": [
    "## Guidance Scale = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bb3a4b-8f91-45a3-a98f-8b17dad1916c",
   "metadata": {},
   "outputs": [],
   "source": [
    "guidance_scale = 1.0\n",
    "\n",
    "generated_images_path = './generated_images/275_1.0'\n",
    "os.makedirs(generated_images_path, exist_ok=True)\n",
    "\n",
    "max_samples = 1500\n",
    "sample_count = 0\n",
    "\n",
    "for batch in val_loader:\n",
    "    if sample_count < 1000:\n",
    "        sample_count += 8\n",
    "        continue\n",
    "    if sample_count >= max_samples:\n",
    "        break\n",
    "\n",
    "    real_images = batch['image'].to(device)\n",
    "    conditions = batch['condition'].to(device)\n",
    "    \n",
    "    for i in range(real_images.size(0)):\n",
    "        if sample_count >= max_samples:\n",
    "            break\n",
    "\n",
    "        age, sex, slice_number = conditions[i][0, 0], conditions[i][0, 1], conditions[i][0, 2]\n",
    "        unconditioned = torch.tensor([[-1, -1, -1]], dtype=torch.float32)  # Shape: (1, 3)\n",
    "        conditioned = torch.tensor([[age, sex, slice_number]], dtype=torch.float32)  # Shape: (1, 3)\n",
    "        conditioning = torch.stack([unconditioned, conditioned], dim=0).to(device)\n",
    "            \n",
    "        noise = torch.randn((1, 1, 96, 128)).to(device)\n",
    "        scheduler.set_timesteps(num_inference_steps=1000)\n",
    "        progress_bar = tqdm(scheduler.timesteps, leave=False)\n",
    "            \n",
    "        for t in progress_bar:\n",
    "            with autocast(enabled=True):\n",
    "                with torch.no_grad():\n",
    "                    noise_input = torch.cat([noise] * 2)\n",
    "                    model_output = model(noise_input, timesteps=torch.Tensor((t,)).to(noise.device), context=conditioning)\n",
    "                    noise_pred_uncond, noise_pred_text = model_output.chunk(2)\n",
    "                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "            noise, _ = scheduler.step(noise_pred, t, noise)\n",
    "            \n",
    "        # Save the generated image\n",
    "        noise_image = noise[0, 0].cpu().numpy()\n",
    "        g_file_name = f\"generated_{sample_count}_{int(slice_number):03d}.png\"\n",
    "        generated_file = os.path.join(generated_images_path, g_file_name)\n",
    "        # Save the image using plt.imsave\n",
    "        plt.imsave(generated_file, noise_image, vmin=0, vmax=1, cmap='gray')\n",
    "        \n",
    "        sample_count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75fef77-afdb-4818-bc8b-a4422efd7395",
   "metadata": {},
   "source": [
    "## Guidance Scale = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c52895-4133-406e-8343-18b2f7319596",
   "metadata": {},
   "outputs": [],
   "source": [
    "guidance_scale = 0.5\n",
    "\n",
    "generated_images_path = './generated_images/275_0.5'\n",
    "os.makedirs(generated_images_path, exist_ok=True)\n",
    "\n",
    "max_samples = 1500\n",
    "sample_count = 0\n",
    "\n",
    "for batch in val_loader:\n",
    "    if sample_count < 1000:\n",
    "        sample_count += 8\n",
    "        continue\n",
    "    if sample_count >= max_samples:\n",
    "        break\n",
    "\n",
    "    real_images = batch['image'].to(device)\n",
    "    conditions = batch['condition'].to(device)\n",
    "    \n",
    "    for i in range(real_images.size(0)):\n",
    "        if sample_count >= max_samples:\n",
    "            break\n",
    "\n",
    "        age, sex, slice_number = conditions[i][0, 0], conditions[i][0, 1], conditions[i][0, 2]\n",
    "        unconditioned = torch.tensor([[-1, -1, -1]], dtype=torch.float32)  # Shape: (1, 3)\n",
    "        conditioned = torch.tensor([[age, sex, slice_number]], dtype=torch.float32)  # Shape: (1, 3)\n",
    "        conditioning = torch.stack([unconditioned, conditioned], dim=0).to(device)\n",
    "            \n",
    "        noise = torch.randn((1, 1, 96, 128)).to(device)\n",
    "        scheduler.set_timesteps(num_inference_steps=1000)\n",
    "        progress_bar = tqdm(scheduler.timesteps, leave=False)\n",
    "            \n",
    "        for t in progress_bar:\n",
    "            with autocast(enabled=True):\n",
    "                with torch.no_grad():\n",
    "                    noise_input = torch.cat([noise] * 2)\n",
    "                    model_output = model(noise_input, timesteps=torch.Tensor((t,)).to(noise.device), context=conditioning)\n",
    "                    noise_pred_uncond, noise_pred_text = model_output.chunk(2)\n",
    "                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "            noise, _ = scheduler.step(noise_pred, t, noise)\n",
    "            \n",
    "        # Save the generated image\n",
    "        noise_image = noise[0, 0].cpu().numpy()\n",
    "        g_file_name = f\"generated_{sample_count}_{int(slice_number):03d}.png\"\n",
    "        generated_file = os.path.join(generated_images_path, g_file_name)\n",
    "        # Save the image using plt.imsave\n",
    "        plt.imsave(generated_file, noise_image, vmin=0, vmax=1, cmap='gray')\n",
    "        \n",
    "        # Save the corresponding real image\n",
    "        real_image = real_images[i, 0].cpu().numpy()\n",
    "        r_file_name = f\"real_{sample_count}_{int(slice_number):03d}.png\"\n",
    "        real_file = os.path.join(real_images_path, r_file_name)\n",
    "        # Save the image using plt.imsave\n",
    "        plt.imsave(real_file, real_image, vmin=0, vmax=1, cmap='gray')\n",
    "        \n",
    "        sample_count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110e0df7-2c74-43f3-b131-4e8627f82bf2",
   "metadata": {},
   "source": [
    " ## Guidance Scale = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b509611b-fb73-4e2f-b7e7-49160230c09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "guidance_scale = 0.0\n",
    "\n",
    "generated_images_path = './generated_images/275_0.0'\n",
    "os.makedirs(generated_images_path, exist_ok=True)\n",
    "\n",
    "max_samples = 1500\n",
    "sample_count = 0\n",
    "\n",
    "for batch in val_loader:\n",
    "    if sample_count < 1000:\n",
    "        sample_count += 8\n",
    "        continue\n",
    "    if sample_count >= max_samples:\n",
    "        break\n",
    "\n",
    "    real_images = batch['image'].to(device)\n",
    "    conditions = batch['condition'].to(device)\n",
    "    \n",
    "    for i in range(real_images.size(0)):\n",
    "        if sample_count >= max_samples:\n",
    "            break\n",
    "\n",
    "        age, sex, slice_number = conditions[i][0, 0], conditions[i][0, 1], conditions[i][0, 2]\n",
    "        unconditioned = torch.tensor([[-1, -1, -1]], dtype=torch.float32)  # Shape: (1, 3)\n",
    "        conditioned = torch.tensor([[age, sex, slice_number]], dtype=torch.float32)  # Shape: (1, 3)\n",
    "        conditioning = torch.stack([unconditioned, conditioned], dim=0).to(device)\n",
    "            \n",
    "        noise = torch.randn((1, 1, 96, 128)).to(device)\n",
    "        scheduler.set_timesteps(num_inference_steps=1000)\n",
    "        progress_bar = tqdm(scheduler.timesteps, leave=False)\n",
    "            \n",
    "        for t in progress_bar:\n",
    "            with autocast(enabled=True):\n",
    "                with torch.no_grad():\n",
    "                    noise_input = torch.cat([noise] * 2)\n",
    "                    model_output = model(noise_input, timesteps=torch.Tensor((t,)).to(noise.device), context=conditioning)\n",
    "                    noise_pred_uncond, noise_pred_text = model_output.chunk(2)\n",
    "                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "            noise, _ = scheduler.step(noise_pred, t, noise)\n",
    "            \n",
    "        # Save the generated image\n",
    "        noise_image = noise[0, 0].cpu().numpy()\n",
    "        g_file_name = f\"generated_{sample_count}_{int(slice_number):03d}.png\"\n",
    "        generated_file = os.path.join(generated_images_path, g_file_name)\n",
    "        # Save the image using plt.imsave\n",
    "        plt.imsave(generated_file, noise_image, vmin=0, vmax=1, cmap='gray')\n",
    "        \n",
    "        # Save the corresponding real image\n",
    "        real_image = real_images[i, 0].cpu().numpy()\n",
    "        r_file_name = f\"real_{sample_count}_{int(slice_number):03d}.png\"\n",
    "        real_file = os.path.join(real_images_path, r_file_name)\n",
    "        # Save the image using plt.imsave\n",
    "        plt.imsave(real_file, real_image, vmin=0, vmax=1, cmap='gray')\n",
    "        \n",
    "        sample_count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ed11e8-d018-46f8-9426-ea69f30c9dc2",
   "metadata": {},
   "source": [
    "## Clean up Data Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5289d1e2-7ab5-4a2c-a8ae-ec0000ae2be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if directory is None:\n",
    "    shutil.rmtree(root_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
