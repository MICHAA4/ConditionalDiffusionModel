{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21394f42-5fbe-4b2e-8bb3-cb4d6defae51",
   "metadata": {},
   "source": [
    "## Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe2b6e6a-2aad-4770-9fbe-34fd5e1dc972",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"import monai\" || pip install -q \"monai-weekly[tqdm]\"\n",
    "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f0a006-5891-463e-ad53-f1c350c2989f",
   "metadata": {},
   "source": [
    "## Setup Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891f2921-5816-4cf5-858c-e516f543a956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    Lambdad,\n",
    "    Resized,\n",
    "    Randomizable,\n",
    "    EnsureChannelFirstd,\n",
    "    ScaleIntensityRanged\n",
    ")\n",
    "from monai.config import print_config\n",
    "from monai.utils import first, set_determinism\n",
    "from monai.data import Dataset, CacheDataset, DataLoader\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from generative.inferers import DiffusionInferer\n",
    "from generative.networks.nets import DiffusionModelUNet\n",
    "from generative.networks.schedulers import DDPMScheduler\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed74f635-60e9-4b07-aa6d-d4df84aec3da",
   "metadata": {},
   "source": [
    "## Setup Data Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caeb808-a81c-4383-ba04-3eaad4e87d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fa21e8-5b14-4f16-bf08-6e2cf4dbbfbc",
   "metadata": {},
   "source": [
    "## Set Deterministic Training for Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a795a093-2baa-49ac-bfc7-fd3ae66bd94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_determinism(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c529070e-78d7-40e7-af20-6380e8c08463",
   "metadata": {},
   "source": [
    "## CamcanDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fa9eccd-1d9c-444d-9581-021be560058d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CamcanDataset(Randomizable, CacheDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir,\n",
    "        csv_file,\n",
    "        section,\n",
    "        transform=None,\n",
    "        seed=0,\n",
    "        val_frac=0.2,\n",
    "        test_frac=0.2,\n",
    "        cache_num=sys.maxsize,\n",
    "        cache_rate=1.0,\n",
    "        num_workers=0,\n",
    "        progress: bool = True,\n",
    "        condition_prob = 0,\n",
    "    ) -> None:\n",
    "        if not os.path.isdir(root_dir):\n",
    "            raise ValueError(\"Root directory root_dir must be a directory.\")\n",
    "        self.root_dir = root_dir\n",
    "        self.csv_file = csv_file\n",
    "        self.section = section\n",
    "        self.val_frac = val_frac\n",
    "        self.test_frac = test_frac\n",
    "        self.condition_prob = condition_prob\n",
    "        self.set_random_state(seed=seed)\n",
    "\n",
    "        data = self._generate_data_list()\n",
    "\n",
    "        CacheDataset.__init__(\n",
    "            self,\n",
    "            data=data,\n",
    "            transform=transform,\n",
    "            cache_num=cache_num,\n",
    "            cache_rate=cache_rate,\n",
    "            num_workers=num_workers,\n",
    "            progress=progress,\n",
    "        )\n",
    "\n",
    "    def randomize(self, data: np.ndarray) -> None:\n",
    "        self.R.shuffle(data)\n",
    "\n",
    "    def _generate_data_list(self):\n",
    "        datalist = []\n",
    "        with open(self.csv_file, mode='r') as file:\n",
    "            reader = csv.DictReader(file)\n",
    "            for row in reader:\n",
    "                image_path = os.path.join(self.root_dir, f\"sub-{row['Subject']}_defaced_T1.nii.gz\")\n",
    "                if not os.path.exists(image_path):\n",
    "                    continue\n",
    "                img = nib.load(image_path)\n",
    "                img_data = img.get_fdata()\n",
    "                for slice_idx in range(img_data.shape[2]//2 - 20, img_data.shape[2]//2 + 20):  # Assuming axial slices\n",
    "                    slice_data = img_data[:,:,slice_idx]\n",
    "                    condition = np.array([\n",
    "                        [int(row['Age'])],\n",
    "                        [int(row['Sex'])],\n",
    "                        [slice_idx]\n",
    "                    ]).reshape((1,3)).astype('float32')\n",
    "                    datalist.append({\n",
    "                        \"image\": slice_data,\n",
    "                        \"condition\": condition\n",
    "                    })\n",
    "        \n",
    "        length = len(datalist)\n",
    "        indices = np.arange(length)\n",
    "        self.randomize(indices)\n",
    "\n",
    "        # train, validation, test split\n",
    "        test_length = int(length * self.test_frac)\n",
    "        val_length = int(length * self.val_frac)\n",
    "        if self.section == \"test\":\n",
    "            section_indices = indices[:test_length]\n",
    "        elif self.section == \"validation\":\n",
    "            section_indices = indices[test_length : test_length + val_length]\n",
    "        elif self.section == \"training\":\n",
    "            section_indices = indices[test_length + val_length :]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f'Unsupported section: {self.section}, available options are [\"training\", \"validation\", \"test\"].'\n",
    "            )\n",
    "        return [datalist[i] for i in section_indices]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data[index]\n",
    "\n",
    "        if random.random() < self.condition_prob:\n",
    "            sample[\"condition\"] = np.array([[-1, -1, -1]])\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ec73a0-5dc4-4dec-b93f-9b1347a3696c",
   "metadata": {},
   "source": [
    "## Setup CamcanDataset and Training and Validation DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c7bb11-05aa-44ba-87a9-6556d9272095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example\n",
    "data_dir = \"./dataset_camcan_sy\"\n",
    "csv_file = \"./phenotype.csv\"\n",
    "\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        EnsureChannelFirstd(keys=[\"image\"], channel_dim='no_channel'),\n",
    "        ScaleIntensityRanged(keys=[\"image\"], a_min=0.0, a_max=255.0, b_min=0.0, b_max=1.0),\n",
    "        Lambdad(keys=[\"condition\"], func=lambda x: torch.tensor(x, dtype=torch.float32)),\n",
    "        Resized(keys=[\"image\"], spatial_size=(96,128)),\n",
    "    ])\n",
    "\n",
    "\n",
    "# Training DataLoader\n",
    "train_ds = CamcanDataset(root_dir=data_dir, csv_file=csv_file, transform=train_transforms, section=\"training\", condition_prob=0.2)\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, num_workers=8, persistent_workers=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b363c2c4-892c-4740-86a5-d6b438031e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transforms = Compose(\n",
    "    [\n",
    "        EnsureChannelFirstd(keys=[\"image\"], channel_dim='no_channel'),\n",
    "        ScaleIntensityRanged(keys=[\"image\"], a_min=0.0, a_max=255.0, b_min=0.0, b_max=1.0),\n",
    "        Lambdad(keys=[\"condition\"], func=lambda x: torch.tensor(x, dtype=torch.float32)),\n",
    "        Resized(keys=[\"image\"], spatial_size=(96,128)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Validation DataLoader\n",
    "val_ds = CamcanDataset(root_dir=data_dir, csv_file=csv_file, transform=val_transforms, section=\"validation\")\n",
    "val_loader = DataLoader(val_ds, batch_size=8, shuffle=False, num_workers=8, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb88a2d-000b-409a-b698-6f27ad7a1277",
   "metadata": {},
   "source": [
    "## Define Network, Scheduler, Optimizer and Inferer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14c8ea30-9b88-4152-b3bc-51019c1c00e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model = DiffusionModelUNet(\n",
    "    spatial_dims=2,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    num_channels=(256, 256, 512),\n",
    "    attention_levels=(False, False, True),\n",
    "    num_res_blocks=2,\n",
    "    num_head_channels=(0, 0, 512),\n",
    "    with_conditioning=True,\n",
    "    cross_attention_dim=3,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=2.5e-5)\n",
    "\n",
    "inferer = DiffusionInferer(scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8608f0dc-71bd-4ba0-93e9-1e2ed9bcff93",
   "metadata": {},
   "source": [
    "## Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52d404c-7d08-4e5e-8303-6ebdbcb71d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the pretrained model\n",
    "pretrained_model_path = 'pretrained_model_275.pth'\n",
    "\n",
    "# Load the state dictionary\n",
    "state_dict = torch.load(pretrained_model_path, map_location=device)\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# If you have a checkpoint with more information (like optimizer state), use this:\n",
    "checkpoint = torch.load('pretrained_model_checkpoint_275.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "start_epoch = checkpoint['epoch']\n",
    "print(start_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce0512c-b172-439e-93d0-017a6e046f0d",
   "metadata": {},
   "source": [
    "## Guidance Scale = 7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe3aac7-ca73-4d65-9287-e7991f003f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "guidance_scale = 7.0\n",
    "\n",
    "generated_images_path = './generated_images/275_7.0'\n",
    "os.makedirs(generated_images_path, exist_ok=True)\n",
    "\n",
    "max_samples = 1800\n",
    "sample_count = 0\n",
    "\n",
    "for batch in val_loader:\n",
    "    if sample_count >= max_samples:\n",
    "        break\n",
    "\n",
    "    real_images = batch['image'].to(device)\n",
    "    conditions = batch['condition'].to(device)\n",
    "    \n",
    "    for i in range(real_images.size(0)):\n",
    "        if sample_count >= max_samples:\n",
    "            break\n",
    "\n",
    "        age, sex, slice_number = conditions[i][0, 0], conditions[i][0, 1], conditions[i][0, 2]\n",
    "        unconditioned = torch.tensor([[-1, -1, -1]], dtype=torch.float32)  # Shape: (1, 3)\n",
    "        conditioned = torch.tensor([[age, sex, slice_number]], dtype=torch.float32)  # Shape: (1, 3)\n",
    "        conditioning = torch.stack([unconditioned, conditioned], dim=0).to(device)\n",
    "            \n",
    "        noise = torch.randn((1, 1, 96, 128)).to(device)\n",
    "        scheduler.set_timesteps(num_inference_steps=1000)\n",
    "        progress_bar = tqdm(scheduler.timesteps, leave=False)\n",
    "            \n",
    "        for t in progress_bar:\n",
    "            with autocast(enabled=True):\n",
    "                with torch.no_grad():\n",
    "                    noise_input = torch.cat([noise] * 2)\n",
    "                    model_output = model(noise_input, timesteps=torch.Tensor((t,)).to(noise.device), context=conditioning)\n",
    "                    noise_pred_uncond, noise_pred_text = model_output.chunk(2)\n",
    "                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "            noise, _ = scheduler.step(noise_pred, t, noise)\n",
    "            \n",
    "        # Save the generated image\n",
    "        noise_image = noise[0, 0].cpu().numpy()\n",
    "        g_file_name = f\"generated_{sample_count}_{int(slice_number):03d}.png\"\n",
    "        generated_file = os.path.join(generated_images_path, g_file_name)\n",
    "        # Save the image using plt.imsave\n",
    "        plt.imsave(generated_file, noise_image, vmin=0, vmax=1, cmap='gray')\n",
    "        \n",
    "        sample_count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8d6bf0-7e58-4626-9a7d-4b21f5a0b91f",
   "metadata": {},
   "source": [
    "## Guidance Scale = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b634e948-1faf-41c1-a058-62fd57cc4a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "guidance_scale = 5.0\n",
    "\n",
    "generated_images_path = './generated_images/275_5.0' \n",
    "os.makedirs(generated_images_path, exist_ok=True)\n",
    "\n",
    "max_samples = 1800\n",
    "sample_count = 0\n",
    "\n",
    "for batch in val_loader:\n",
    "    if sample_count >= max_samples:\n",
    "        break\n",
    "\n",
    "    real_images = batch['image'].to(device)\n",
    "    conditions = batch['condition'].to(device)\n",
    "    \n",
    "    for i in range(real_images.size(0)):\n",
    "        if sample_count >= max_samples:\n",
    "            break\n",
    "\n",
    "        age, sex, slice_number = conditions[i][0, 0], conditions[i][0, 1], conditions[i][0, 2]\n",
    "        unconditioned = torch.tensor([[-1, -1, -1]], dtype=torch.float32)  # Shape: (1, 3)\n",
    "        conditioned = torch.tensor([[age, sex, slice_number]], dtype=torch.float32)  # Shape: (1, 3)\n",
    "        conditioning = torch.stack([unconditioned, conditioned], dim=0).to(device)\n",
    "            \n",
    "        noise = torch.randn((1, 1, 96, 128)).to(device)\n",
    "        scheduler.set_timesteps(num_inference_steps=1000)\n",
    "        progress_bar = tqdm(scheduler.timesteps, leave=False)\n",
    "            \n",
    "        for t in progress_bar:\n",
    "            with autocast(enabled=True):\n",
    "                with torch.no_grad():\n",
    "                    noise_input = torch.cat([noise] * 2)\n",
    "                    model_output = model(noise_input, timesteps=torch.Tensor((t,)).to(noise.device), context=conditioning)\n",
    "                    noise_pred_uncond, noise_pred_text = model_output.chunk(2)\n",
    "                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "            noise, _ = scheduler.step(noise_pred, t, noise)\n",
    "            \n",
    "        # Save the generated image\n",
    "        noise_image = noise[0, 0].cpu().numpy()\n",
    "        g_file_name = f\"generated_{sample_count}_{int(slice_number):03d}.png\"\n",
    "        generated_file = os.path.join(generated_images_path, g_file_name)\n",
    "        # Save the image using plt.imsave\n",
    "        plt.imsave(generated_file, noise_image, vmin=0, vmax=1, cmap='gray')\n",
    "        \n",
    "        sample_count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1fa37c-d0e4-48f8-a694-3753c77db8c8",
   "metadata": {},
   "source": [
    "## Guidance Scale = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bb3a4b-8f91-45a3-a98f-8b17dad1916c",
   "metadata": {},
   "outputs": [],
   "source": [
    "guidance_scale = 1.0\n",
    "\n",
    "generated_images_path = './generated_images/275_1.0'\n",
    "os.makedirs(generated_images_path, exist_ok=True)\n",
    "\n",
    "max_samples = 1800\n",
    "sample_count = 0\n",
    "\n",
    "for batch in val_loader:\n",
    "    if sample_count >= max_samples:\n",
    "        break\n",
    "\n",
    "    real_images = batch['image'].to(device)\n",
    "    conditions = batch['condition'].to(device)\n",
    "    \n",
    "    for i in range(real_images.size(0)):\n",
    "        if sample_count >= max_samples:\n",
    "            break\n",
    "\n",
    "        age, sex, slice_number = conditions[i][0, 0], conditions[i][0, 1], conditions[i][0, 2]\n",
    "        unconditioned = torch.tensor([[-1, -1, -1]], dtype=torch.float32)  # Shape: (1, 3)\n",
    "        conditioned = torch.tensor([[age, sex, slice_number]], dtype=torch.float32)  # Shape: (1, 3)\n",
    "        conditioning = torch.stack([unconditioned, conditioned], dim=0).to(device)\n",
    "            \n",
    "        noise = torch.randn((1, 1, 96, 128)).to(device)\n",
    "        scheduler.set_timesteps(num_inference_steps=1000)\n",
    "        progress_bar = tqdm(scheduler.timesteps, leave=False)\n",
    "            \n",
    "        for t in progress_bar:\n",
    "            with autocast(enabled=True):\n",
    "                with torch.no_grad():\n",
    "                    noise_input = torch.cat([noise] * 2)\n",
    "                    model_output = model(noise_input, timesteps=torch.Tensor((t,)).to(noise.device), context=conditioning)\n",
    "                    noise_pred_uncond, noise_pred_text = model_output.chunk(2)\n",
    "                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "            noise, _ = scheduler.step(noise_pred, t, noise)\n",
    "            \n",
    "        # Save the generated image\n",
    "        noise_image = noise[0, 0].cpu().numpy()\n",
    "        g_file_name = f\"generated_{sample_count}_{int(slice_number):03d}.png\"\n",
    "        generated_file = os.path.join(generated_images_path, g_file_name)\n",
    "        # Save the image using plt.imsave\n",
    "        plt.imsave(generated_file, noise_image, vmin=0, vmax=1, cmap='gray')\n",
    "        \n",
    "        sample_count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75fef77-afdb-4818-bc8b-a4422efd7395",
   "metadata": {},
   "source": [
    "## Guidance Scale = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c52895-4133-406e-8343-18b2f7319596",
   "metadata": {},
   "outputs": [],
   "source": [
    "guidance_scale = 0.5\n",
    "\n",
    "generated_images_path = './generated_images/275_0.5'\n",
    "os.makedirs(generated_images_path, exist_ok=True)\n",
    "\n",
    "max_samples = 1800\n",
    "sample_count = 0\n",
    "\n",
    "for batch in val_loader:\n",
    "    if sample_count >= max_samples:\n",
    "        break\n",
    "\n",
    "    real_images = batch['image'].to(device)\n",
    "    conditions = batch['condition'].to(device)\n",
    "    \n",
    "    for i in range(real_images.size(0)):\n",
    "        if sample_count >= max_samples:\n",
    "            break\n",
    "\n",
    "        age, sex, slice_number = conditions[i][0, 0], conditions[i][0, 1], conditions[i][0, 2]\n",
    "        unconditioned = torch.tensor([[-1, -1, -1]], dtype=torch.float32)  # Shape: (1, 3)\n",
    "        conditioned = torch.tensor([[age, sex, slice_number]], dtype=torch.float32)  # Shape: (1, 3)\n",
    "        conditioning = torch.stack([unconditioned, conditioned], dim=0).to(device)\n",
    "            \n",
    "        noise = torch.randn((1, 1, 96, 128)).to(device)\n",
    "        scheduler.set_timesteps(num_inference_steps=1000)\n",
    "        progress_bar = tqdm(scheduler.timesteps, leave=False)\n",
    "            \n",
    "        for t in progress_bar:\n",
    "            with autocast(enabled=True):\n",
    "                with torch.no_grad():\n",
    "                    noise_input = torch.cat([noise] * 2)\n",
    "                    model_output = model(noise_input, timesteps=torch.Tensor((t,)).to(noise.device), context=conditioning)\n",
    "                    noise_pred_uncond, noise_pred_text = model_output.chunk(2)\n",
    "                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "            noise, _ = scheduler.step(noise_pred, t, noise)\n",
    "            \n",
    "        # Save the generated image\n",
    "        noise_image = noise[0, 0].cpu().numpy()\n",
    "        g_file_name = f\"generated_{sample_count}_{int(slice_number):03d}.png\"\n",
    "        generated_file = os.path.join(generated_images_path, g_file_name)\n",
    "        # Save the image using plt.imsave\n",
    "        plt.imsave(generated_file, noise_image, vmin=0, vmax=1, cmap='gray')\n",
    "        \n",
    "        # Save the corresponding real image\n",
    "        real_image = real_images[i, 0].cpu().numpy()\n",
    "        r_file_name = f\"real_{sample_count}_{int(slice_number):03d}.png\"\n",
    "        real_file = os.path.join(real_images_path, r_file_name)\n",
    "        # Save the image using plt.imsave\n",
    "        plt.imsave(real_file, real_image, vmin=0, vmax=1, cmap='gray')\n",
    "        \n",
    "        sample_count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110e0df7-2c74-43f3-b131-4e8627f82bf2",
   "metadata": {},
   "source": [
    " ## Guidance Scale = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b509611b-fb73-4e2f-b7e7-49160230c09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "guidance_scale = 0.0\n",
    "\n",
    "generated_images_path = './generated_images/275_0.0'\n",
    "os.makedirs(generated_images_path, exist_ok=True)\n",
    "\n",
    "max_samples = 1800\n",
    "sample_count = 0\n",
    "\n",
    "for batch in val_loader:\n",
    "    if sample_count >= max_samples:\n",
    "        break\n",
    "\n",
    "    real_images = batch['image'].to(device)\n",
    "    conditions = batch['condition'].to(device)\n",
    "    \n",
    "    for i in range(real_images.size(0)):\n",
    "        if sample_count >= max_samples:\n",
    "            break\n",
    "\n",
    "        age, sex, slice_number = conditions[i][0, 0], conditions[i][0, 1], conditions[i][0, 2]\n",
    "        unconditioned = torch.tensor([[-1, -1, -1]], dtype=torch.float32)  # Shape: (1, 3)\n",
    "        conditioned = torch.tensor([[age, sex, slice_number]], dtype=torch.float32)  # Shape: (1, 3)\n",
    "        conditioning = torch.stack([unconditioned, conditioned], dim=0).to(device)\n",
    "            \n",
    "        noise = torch.randn((1, 1, 96, 128)).to(device)\n",
    "        scheduler.set_timesteps(num_inference_steps=1000)\n",
    "        progress_bar = tqdm(scheduler.timesteps, leave=False)\n",
    "            \n",
    "        for t in progress_bar:\n",
    "            with autocast(enabled=True):\n",
    "                with torch.no_grad():\n",
    "                    noise_input = torch.cat([noise] * 2)\n",
    "                    model_output = model(noise_input, timesteps=torch.Tensor((t,)).to(noise.device), context=conditioning)\n",
    "                    noise_pred_uncond, noise_pred_text = model_output.chunk(2)\n",
    "                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "            noise, _ = scheduler.step(noise_pred, t, noise)\n",
    "            \n",
    "        # Save the generated image\n",
    "        noise_image = noise[0, 0].cpu().numpy()\n",
    "        g_file_name = f\"generated_{sample_count}_{int(slice_number):03d}.png\"\n",
    "        generated_file = os.path.join(generated_images_path, g_file_name)\n",
    "        # Save the image using plt.imsave\n",
    "        plt.imsave(generated_file, noise_image, vmin=0, vmax=1, cmap='gray')\n",
    "        \n",
    "        # Save the corresponding real image\n",
    "        real_image = real_images[i, 0].cpu().numpy()\n",
    "        r_file_name = f\"real_{sample_count}_{int(slice_number):03d}.png\"\n",
    "        real_file = os.path.join(real_images_path, r_file_name)\n",
    "        # Save the image using plt.imsave\n",
    "        plt.imsave(real_file, real_image, vmin=0, vmax=1, cmap='gray')\n",
    "        \n",
    "        sample_count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ed11e8-d018-46f8-9426-ea69f30c9dc2",
   "metadata": {},
   "source": [
    "## Clean up Data Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5289d1e2-7ab5-4a2c-a8ae-ec0000ae2be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if directory is None:\n",
    "    shutil.rmtree(root_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
